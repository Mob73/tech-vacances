{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46997dc0",
   "metadata": {},
   "source": [
    "##  Web scraping en Python\n",
    "\n",
    "Le **web scraping** consiste à extraire automatiquement des données depuis des pages web.\n",
    "\n",
    "### Étapes principales\n",
    "\n",
    "1. **Envoyer une requête HTTP**  \n",
    "   Utiliser une bibliothèque comme `requests` pour télécharger le contenu d’une page web.\n",
    "\n",
    "2. **Analyser le contenu HTML**  \n",
    "   Utiliser `BeautifulSoup` pour parser et naviguer dans le code HTML.\n",
    "\n",
    "3. **Extraire les données**  \n",
    "   Repérer les balises ou attributs qui contiennent les informations recherchées et les extraire.\n",
    "\n",
    "4. **Stocker les données**  \n",
    "   Sauvegarder les données extraites dans un fichier (CSV, Excel, base de données, etc.).\n",
    "\n",
    "### Bibliothèques courantes\n",
    "\n",
    "- `requests` : pour télécharger les pages web.\n",
    "- `BeautifulSoup` : pour analyser et extraire les données du HTML.\n",
    "- `pandas` : pour organiser et stocker les données.\n",
    "\n",
    "### Exemple simple\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraire tous les titres h1\n",
    "titres = [h1.text for h1 in soup.find_all(\"h1\")]\n",
    "print(titres)\n",
    "```\n",
    "\n",
    "### Bonnes pratiques\n",
    "\n",
    "- Respecter les conditions d’utilisation du site (robots.txt).\n",
    "- Ne pas surcharger les serveurs (utiliser des délais entre les requêtes).\n",
    "- Vérifier la légalité du scraping selon le site et le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0f554",
   "metadata": {},
   "source": [
    "#### Processus \n",
    "\n",
    "```python\n",
    "   \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    #### Requête GET basique\n",
    "    response = requests.get('https://www.example.com')\n",
    "```\n",
    "###### Requête GET\n",
    "requests.get(url)\n",
    "\n",
    "###### Requête POST\n",
    "requests.post(url, data={'key': 'value'})\n",
    "\n",
    "###### Requête PUT\n",
    "requests.put(url, data={'key': 'value'})\n",
    "\n",
    "###### Requête DELETE\n",
    "requests.delete(url)\n",
    "\n",
    "###### Requête HEAD (récupère seulement les en-têtes)\n",
    "requests.head(url)\n",
    "### les reponses\n",
    "\n",
    "```python\n",
    "    response.status_code  # Code de statut HTTP (200, 404, etc.)\n",
    "    response.text        # Corps de la réponse en string\n",
    "    response.content     # Corps de la réponse en bytes\n",
    "    response.json()      # Parse la réponse JSON\n",
    "    response.headers     # En-têtes de la réponse\n",
    "    response.cookies     # Cookies reçus\n",
    "    response.url         # URL finale après redirections\n",
    "    response.history     # Historique des redirections\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Gestion des erreurs \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "    try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()  # Lève une HTTPError pour les codes d'erreur\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Échec de la requête : {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42716792",
   "metadata": {},
   "source": [
    "### Les bases de BeautifulSoup\n",
    "```python\n",
    "    html = \"la réponse text reçue de request \"#response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser') # Parseurs disponibles : 'html.parser', 'lxml', 'html5lib'\n",
    "\n",
    "\n",
    "    # Accéder aux balises\n",
    "soup.title          # Balise <title>\n",
    "soup.title.name     # 'title'\n",
    "soup.title.string   # Texte dans la balise title\n",
    "soup.p              # Première balise <p>\n",
    "\n",
    "# Naviguer vers le bas\n",
    "soup.head           # Balise <head>\n",
    "soup.body.b         # Première balise <b> dans body\n",
    "\n",
    "# Naviguer vers le haut\n",
    "tag = soup.p\n",
    "tag.parent          # Balise parente\n",
    "\n",
    "# Naviguer latéralement\n",
    "tag.next_sibling    # Balise suivante au même niveau\n",
    "tag.previous_sibling # Balise précédente au même niveau\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Trouver le premier élément correspondant\n",
    "soup.find('div')             # Première div\n",
    "soup.find(id='main')         # Premier élément avec id='main'\n",
    "soup.find(class_='header')   # Premier élément avec class 'header'\n",
    "soup.find('a', href=True)    # Premier <a> avec attribut href\n",
    "\n",
    "# Trouver tous les éléments correspondants\n",
    "soup.find_all('p')           # Toutes les balises <p>\n",
    "soup.find_all('a', class_='external')  # Tous les liens externes\n",
    "soup.find_all(text='Bonjour')  # Tous les nœuds texte contenant 'Bonjour'\n",
    "\n",
    "# Sélecteurs CSS\n",
    "soup.select('div.content')   # Toutes les divs avec class 'content'\n",
    "soup.select('#footer')       # Élément avec id 'footer'\n",
    "soup.select('a[href^=\"https\"]')  # Liens commençant par https\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modifier les attributs\n",
    "tag['class'] = 'nouvelle-classe'\n",
    "del tag['id']\n",
    "\n",
    "# Modifier le texte\n",
    "tag.string = \"Nouveau texte\"\n",
    "\n",
    "# Créer de nouvelles balises\n",
    "new_tag = soup.new_tag('a', href='http://example.com')\n",
    "new_tag.string = \"Cliquez ici\"\n",
    "\n",
    "# Ajouter des balises\n",
    "tag.append(new_tag)\n",
    "tag.insert(0, new_tag)  # Insérer au début\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2246dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Search code, repositories, users, issues, pull requests...', '\\n        Provide feedback\\n      ', '\\n        Saved searches\\n      ', 'Build and ship software on a single, collaborative platform']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraire tous les titres h1\n",
    "titres = [h1.text for h1 in soup.find_all(\"h1\")]\n",
    "print(titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df0a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statut de la réponse : 200\n"
     ]
    }
   ],
   "source": [
    "# URL de la page à scraper\n",
    "url = \"https://fr.wikipedia.org/wiki/Liste_des_langages_de_programmation\"\n",
    "\n",
    "# En-têtes pour simuler un navigateur\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Faire la requête GET\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Lève une exception pour les codes 4XX/5XX\n",
    "    print(f\"Statut de la réponse : {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erreur lors de la requête : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43dadba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre de la page : Liste de langages de programmation — Wikipédia\n"
     ]
    }
   ],
   "source": [
    "# Créer un objet BeautifulSoup à partir du contenu HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Afficher le titre de la page pour vérifier\n",
    "print(f\"Titre de la page : {soup.title.string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f7d99ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 premiers langages trouvés :\n"
     ]
    }
   ],
   "source": [
    "# Trouver tous les éléments <li> dans la page\n",
    "all_languages = []\n",
    "\n",
    "# La page Wikipedia a plusieurs listes, nous ciblons celles dans les sections principales\n",
    "sections = soup.find_all('div', class_='div-col')\n",
    "\n",
    "for section in sections:\n",
    "    languages = section.find_all('li')\n",
    "    for lang in languages:\n",
    "        # Nettoyer le texte et extraire le nom du langage\n",
    "        language_name = lang.get_text().strip()\n",
    "        # Certains éléments contiennent des notes entre parenthèses\n",
    "        if '(' in language_name:\n",
    "            language_name = language_name.split('(')[0].strip()\n",
    "        all_languages.append(language_name)\n",
    "\n",
    "# Afficher les 10 premiers langages\n",
    "print(\"10 premiers langages trouvés :\")\n",
    "for lang in all_languages[:10]:\n",
    "    print(f\"- {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e58bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de langages trouvés : 0\n",
      "\n",
      "Quelques statistiques :\n",
      "       Langage\n",
      "count        0\n",
      "unique       0\n",
      "top        NaN\n",
      "freq       NaN\n"
     ]
    }
   ],
   "source": [
    "# Créer un DataFrame pandas pour analyser les données\n",
    "df_languages = pd.DataFrame(all_languages, columns=['Langage'])\n",
    "\n",
    "# Statistiques de base\n",
    "print(f\"Nombre total de langages trouvés : {len(df_languages)}\")\n",
    "print(\"\\nQuelques statistiques :\")\n",
    "print(df_languages.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
